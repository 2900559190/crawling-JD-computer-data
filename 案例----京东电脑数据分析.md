# 案例----京东电脑数据分析

> ```python
> """
> 爬虫爬取网页数据，需要登陆的情况
> 1. 先用selenium库模拟登录，登录之后进入需要批量爬取数据的网页（第一页即可）
> 2. 获取当前页的cookie，一般取cookie的某个key的值进行拼接，京东的是cookie[name]的值进行拼接
> 3. 构造需要爬取的URL列表（需要浏览器抓包分析）
> 4. 构造header（请求头）， 传入重新拼接的cookie和使用随机的user_agent
> 5. 使用requests库的get方法，传入url和构造的header
> 6. 开始解析网页，获取数据
> 7. 数据库使用的是sqlite的话，插入数据时需要上锁 --> (针对的是多线程爬取数据的情况)
> """
> ```

## 需求分析（MVC）：

1. 控制管理层（Controller）
   - Crawling类：处理爬取数据（包含登录）
     - login：模拟登录京东
     - get：获取cookie后爬取构造的商品的URL的商品信息
     - parse_page：解析URL的信息
     - get_url_lst：构造产品的URL列表
   - tools.py：
     - get_key：获取特定的字符，用于清理解析的数据
2. 数据存储模块（Model）
   - Computer类：sqlite数据库的相关操作
     - create：创建一张表
     - insert：向表中插入数据
     - select：查询表中数据
     - close：关闭数据库相关连接
   - get_connection：获取sqlite的连接
3. 数据分析（View）
   - Draw类：绘制相关图
     - histogram：绘制柱状图（频数直方图）

